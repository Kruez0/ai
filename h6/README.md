# Macrograd w/ Nmist & Cross Entropy Loss
>I got confused but I understand now. This was done by looking laoshi's mmist and cross entrophy loss.

## Definition   

MNIST is a dataset of handwritten digits used for training image classification models.   

Cross-Entropy Loss is a metric for evaluating how well a modelâ€™s predicted probability distribution matches the actual labels.

## Results
From the result below we can see that the loss decrease after each 1000 steps.
```
loss in step 0 is 14.585225403969448
loss in step 1000 is 0.6754852050100334
loss in step 2000 is 0.598522013503464
loss in step 3000 is 0.5109825452082237
loss in step 4000 is 0.45423169543569925
loss in step 5000 is 0.4264593417545452
loss in step 6000 is 0.4402184396035847
loss in step 7000 is 0.39692396971887417
loss in step 8000 is 0.3637056835403614
loss in step 9000 is 0.3655059733416944
loss in step 10000 is 0.34539645076407477
loss in step 11000 is 0.3545388555731599
loss in step 12000 is 0.3325216939637075
loss in step 13000 is 0.33571534957344695
loss in step 14000 is 0.3305550448928959
loss in step 15000 is 0.3130073407586916
loss in step 16000 is 0.3084408976664681
loss in step 17000 is 0.3433473550016504
loss in step 18000 is 0.314702896843841
loss in step 19000 is 0.3086663179949378
loss in step 19999 is 0.3069858031550517
accuracy on test data is 90.92 %
```
